{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP3bDaTNXg4ZD7waUHq7BY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "nHMvb5otFfHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 2: Sensado y análisis de video"
      ],
      "metadata": {
        "id": "g3dDKVY8bXGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esta práctica se generó un dataset que contenía 150 videos. Estos videos contienen movimientos utilizados en la escala de evaluación Fugl-Meyer para extremidades superiores. Cada uno de los 10 sujetos realizó 5 movimientos en 3 modalidades (*full*, *partial* y *none*, o completa, parcial y nula). El propósito de la práctica es generar un clasificador que pueda correctamente clasificar un video nuevo como un movimiento *full*, *partial* o *none*."
      ],
      "metadata": {
        "id": "-D8GvsyKcm8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos importando las librerías a utilizar."
      ],
      "metadata": {
        "id": "GErAgVI_dz4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import copy\n",
        "import itertools\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import glob\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "SrdhJBmyiC-W"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos la librería que nos permite cargar datos desde Google Drive, donde tenemos guardados nuestros videos."
      ],
      "metadata": {
        "id": "5syCQyHylA5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n9AhdG7mxHSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f83bee-058f-4356-c650-df3a79dfc789"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establecemos las localizaciones de los videos. Para motivos de entrenamiento se han separado en 5 carpetas según el movimiento, cada una con sus subcarpetas para los movimientos completos, parciales y nulos."
      ],
      "metadata": {
        "id": "GNtUrjxfd7ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = \"/content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento1/\"\n",
        "path2 = \"/content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento2/\"\n",
        "path3 = \"/content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento3/\"\n",
        "path4 = \"/content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento4/\"\n",
        "path5 = \"/content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento5/\""
      ],
      "metadata": {
        "id": "-q06abKaILD4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"Full\", \"Partial\", \"None\"]"
      ],
      "metadata": {
        "id": "f-7B7T2YP5yk"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_paths = [\n",
        "    path1, path2, path3, path4, path5\n",
        "]"
      ],
      "metadata": {
        "id": "x87M7AOLQBXj"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utiliza MediaPipe para extraer los *landmarks*."
      ],
      "metadata": {
        "id": "cA33wyYdeLyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)"
      ],
      "metadata": {
        "id": "WPvVkl2cKCOt"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pose_landmarks(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    landmarks_list = []\n",
        "\n",
        "    # Se verifica que el video cargó correctamente\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return np.array([])\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Se convierten los frames a RGB\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Se procesan los frames con MediaPipe Pose\n",
        "        results = pose.process(rgb_frame)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            # Extrae los landmarks de pose\n",
        "            landmarks = np.array([[lm.x, lm.y, lm.z] for lm in results.pose_landmarks.landmark]).flatten()\n",
        "            landmarks_list.append(landmarks)\n",
        "        else:\n",
        "            print(f\"Warning: No landmarks detected in frame {frame_count} of {video_path}\")\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(landmarks_list)"
      ],
      "metadata": {
        "id": "H3q4zKvpJ7vV"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente función se encargará de cargar el dataset para cada una de nuestras carpetas (cada movimiento tendrá su dataset)."
      ],
      "metadata": {
        "id": "FQfKvkO0lJeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_for_path(base_path, class_names):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        class_folder = os.path.join(base_path, class_name)\n",
        "        if os.path.exists(class_folder):\n",
        "            video_files = [f for f in os.listdir(class_folder) if f.lower().endswith(('.mov', '.mp4', '.avi'))]\n",
        "            for video_file in video_files:\n",
        "                video_path = os.path.join(class_folder, video_file)\n",
        "                landmarks = extract_pose_landmarks(video_path)\n",
        "\n",
        "                # Verifica que los landmarks fueron extraídos\n",
        "                if landmarks.size > 0:\n",
        "                    # Calculate a single feature vector for the video (e.g., mean)\n",
        "                    video_features = np.mean(landmarks, axis=0)  # Average landmarks across all frames\n",
        "\n",
        "                    X.append(video_features)\n",
        "                    y.append(class_idx)\n",
        "                else:\n",
        "                    print(f\"Warning: No landmarks extracted from {video_file}. Skipping...\")\n",
        "        else:\n",
        "            print(f\"Warning: Subfolder '{class_name}' not found in '{base_path}'. Skipping...\")\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "i2N0a8uLPt4w"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizaron RandomForest, KNN, DecisionTree, Regresión Logística y Naive Bayes como clasificadores para entrenar nuestros modelos. Se entrenó un modelo con cada uno de los movimientos. Luego se escogió según sus puntajes el mejor de los modelos, con el cual se haría la prueba externa."
      ],
      "metadata": {
        "id": "G3GMeD8zeuRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    GaussianNB(),\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    KNeighborsClassifier(),\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    LogisticRegression(random_state=42, max_iter=1000)\n",
        "]\n",
        "\n",
        "for i, base_path in enumerate(base_paths):\n",
        "    print(f\"Training classifier for {base_path}...\")\n",
        "\n",
        "    # Se carga el dataset para la localización actual\n",
        "    X, y = load_dataset_for_path(base_path, class_names)\n",
        "\n",
        "    # Selecciona el clasificador para la localización actual\n",
        "    if i < len(classifiers):  # Verifica que haya un clasificador definido para esta localización\n",
        "        model = classifiers[i]\n",
        "    else:\n",
        "        # RandomForest por defecto si no se especificó un clasificador en concreto\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Se segmentan los datos en datos de entrenamiento y de prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Remove to_categorical for GaussianNB and LogisticRegression\n",
        "    if isinstance(model, (GaussianNB, LogisticRegression)):\n",
        "        # GaussianNB and LogisticRegression expect a 1D array for y\n",
        "        pass  # No need to change y_train and y_test\n",
        "    else:\n",
        "        # Other models might work with one-hot encoding\n",
        "        y_train = to_categorical(y_train)\n",
        "        y_test = to_categorical(y_test)\n",
        "\n",
        "    # Entrena el modelo\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evalúa el modelo\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    #If the model is GaussianNB or LogisticRegression then reverse the one-hot encoding to calculate the accuracy\n",
        "    if isinstance(model, (GaussianNB, LogisticRegression)):\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "    else:\n",
        "        accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "\n",
        "    print(f\"Test Accuracy for {base_path}: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Guarda el modelo\n",
        "    model_save_path = os.path.join(base_path, f\"video_classifier_model_{i+1}.pkl\")\n",
        "    joblib.dump(model, model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6Cgy2LeQM76",
        "outputId": "6bf75b9c-bd77-4bdd-eab9-78de7dba1506"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento1/...\n",
            "Test Accuracy for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento1/: 66.67%\n",
            "Model saved to /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento1/video_classifier_model_1.pkl\n",
            "\n",
            "Training classifier for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento2/...\n",
            "Test Accuracy for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento2/: 66.67%\n",
            "Model saved to /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento2/video_classifier_model_2.pkl\n",
            "\n",
            "Training classifier for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento3/...\n",
            "Test Accuracy for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento3/: 16.67%\n",
            "Model saved to /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento3/video_classifier_model_3.pkl\n",
            "\n",
            "Training classifier for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento4/...\n",
            "Test Accuracy for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento4/: 66.67%\n",
            "Model saved to /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento4/video_classifier_model_4.pkl\n",
            "\n",
            "Training classifier for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento5/...\n",
            "Test Accuracy for /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento5/: 0.00%\n",
            "Model saved to /content/drive/MyDrive/CDSI/Practica02/Videos/Movimiento5/video_classifier_model_5.pkl\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prueba con video nuevo"
      ],
      "metadata": {
        "id": "BT_d08YUmjVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "video_file_name = list(uploaded.keys())[0]  # Get the name of the uploaded file\n",
        "\n",
        "# Extract features from the uploaded video\n",
        "landmarks = extract_pose_landmarks(video_file_name)\n",
        "\n",
        "# Check if landmarks were extracted\n",
        "if landmarks.size == 0:\n",
        "    print(\"No landmarks detected in the video.\")\n",
        "else:\n",
        "    # Load the trained model\n",
        "    # Load the trained model using joblib.load()\n",
        "    model_path = \"/content/drive/My Drive/CDSI/Practica02/Videos/Movimiento1/video_classifier_model_1.pkl\"\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "    # Normalize the landmarks (if required)\n",
        "    # Example: If you used StandardScaler during training, load and apply it here\n",
        "    # from sklearn.preprocessing import StandardScaler\n",
        "    # scaler = StandardScaler()\n",
        "    # landmarks = scaler.transform(landmarks)\n",
        "\n",
        "    # Predict the class\n",
        "    predictions = model.predict(landmarks)\n",
        "    predicted_class_idx = np.argmax(np.mean(predictions, axis=0))  # Average predictions over all frames\n",
        "    class_names = [\"full\", \"partial\", \"none\"]\n",
        "    predicted_class = class_names[predicted_class_idx]\n",
        "\n",
        "    # Display the result\n",
        "    print(f\"The uploaded video is classified as: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "35RhPcH7l8IQ",
        "outputId": "6285fcd0-987c-4372-e6c0-cd1e6b177114"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f80c9452-c3c4-4a81-927f-fa1d9a8c7c24\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f80c9452-c3c4-4a81-927f-fa1d9a8c7c24\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Video 2025-02-12 at 9.57.26 AM.mp4 to WhatsApp Video 2025-02-12 at 9.57.26 AM (1).mp4\n",
            "The uploaded video is classified as: full\n"
          ]
        }
      ]
    }
  ]
}